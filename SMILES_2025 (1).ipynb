{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_2Wmj7jvT4Nd",
        "outputId": "7e4f63de-15b3-4cf5-863a-76308873a045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Mako, colorlog, alembic, optuna\n",
            "Successfully installed Mako-1.3.10 alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n",
            "Collecting gudhi\n",
            "  Downloading gudhi-3.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from gudhi) (1.26.4)\n",
            "Downloading gudhi-3.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gudhi\n",
            "Successfully installed gudhi-3.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install optuna\n",
        "!pip install torch_geometric\n",
        "!pip install gudhi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main model. For different types change flags USE_PH or USE_SPECTRAL"
      ],
      "metadata": {
        "id": "2Z5rzAoAT9bS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import optuna\n",
        "\n",
        "from torch_geometric.datasets import WebKB, WikipediaNetwork, Planetoid, Actor\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "import gudhi\n",
        "from gudhi.representations import PersistenceImage\n",
        "USE_PH = True\n",
        "USE_SPECTRAL = True\n",
        "\n",
        "def compute_local_PH(x, edge_index, k=5, maxdim=1):\n",
        "    row, col = edge_index\n",
        "    N = x.size(0)\n",
        "    x_np = x.detach().cpu().numpy()\n",
        "    neighs = [[] for _ in range(N)]\n",
        "    for i,j in zip(row.tolist(), col.tolist()):\n",
        "        neighs[i].append(j)\n",
        "\n",
        "    diagrams = []\n",
        "    for i in range(N):\n",
        "        patch_pts = [x_np[i]]\n",
        "        for j in neighs[i][:k]:\n",
        "            patch_pts.append(x_np[j])\n",
        "        patch = np.stack(patch_pts, axis=0)\n",
        "        rips = gudhi.RipsComplex(points=patch)\n",
        "        st = rips.create_simplex_tree(max_dimension=maxdim)\n",
        "        st.persistence()\n",
        "        diag = st.persistence_intervals_in_dimension(1)\n",
        "        if len(diag)==0:\n",
        "            diag = np.array([[0.0,0.0]])\n",
        "        diagrams.append(diag)\n",
        "    return diagrams\n",
        "\n",
        "def get_persistence_vectors(diagrams, resolution=(5,5)):\n",
        "    PI = PersistenceImage(bandwidth=1.0,\n",
        "                          weight=lambda pt: pt[1]-pt[0],\n",
        "                          resolution=resolution)\n",
        "    vecs = PI.fit_transform(diagrams)\n",
        "    vecs = np.nan_to_num(vecs, nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "    return torch.tensor(vecs, dtype=torch.float)\n",
        "\n",
        "def laplacian_regularization(edge_index, num_nodes):\n",
        "    row, col = edge_index\n",
        "    deg = torch.bincount(row, minlength=num_nodes).float()\n",
        "    D_inv_sqrt = torch.diag(torch.pow(deg.clamp(min=1), -0.5))\n",
        "    A = torch.zeros(num_nodes, num_nodes, device=edge_index.device)\n",
        "    A[row, col] = 1\n",
        "    return torch.eye(num_nodes, device=edge_index.device) - D_inv_sqrt @ A @ D_inv_sqrt\n",
        "\n",
        "class DeepSheafLayer(nn.Module):\n",
        "    def __init__(self, d, f_dim):\n",
        "        super().__init__()\n",
        "        self.d, self.f = d, f_dim\n",
        "        self.lin = nn.Linear(d * f_dim, d * f_dim)\n",
        "        self.ln  = nn.LayerNorm([d, f_dim])\n",
        "    def forward(self, x, edge_index):\n",
        "        N = x.size(0)\n",
        "        flat = x.view(N, -1)\n",
        "        outf = self.lin(flat).view(N, self.d, self.f)\n",
        "        row, col = edge_index\n",
        "        agg = torch.zeros_like(outf)\n",
        "        agg.index_add_(0, row, outf[col])\n",
        "        deg = torch.bincount(row, minlength=N).clamp(min=1).float().view(-1,1,1)\n",
        "        agg = agg / deg\n",
        "        return self.ln(F.relu(agg))\n",
        "\n",
        "class DeepSheafNet(nn.Module):\n",
        "    def __init__(self, in_dim, d, f_dim, out_dim, depth=4, spectral_weight=1e-5, ph_dim=25):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Linear(in_dim, d)\n",
        "        self.spectral_weight = spectral_weight\n",
        "        node_dim = d + (ph_dim if USE_PH else 0)\n",
        "        self.layers = nn.ModuleList([DeepSheafLayer(d, f_dim) for _ in range(depth)])\n",
        "        self.decoder = nn.Sequential(nn.Flatten(), nn.Linear(d*f_dim, out_dim))\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        nf = self.encoder(x)\n",
        "        nf = torch.nan_to_num(nf, nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "        xs = nf.unsqueeze(2).repeat(1,1,self.layers[0].f)\n",
        "\n",
        "        if USE_PH:\n",
        "            phd = compute_local_PH(nf, edge_index)\n",
        "            phv = get_persistence_vectors(phd).to(x.device)\n",
        "            phv = torch.nan_to_num(phv, nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "            node_feat = torch.cat([nf, phv], dim=1)\n",
        "        else:\n",
        "            node_feat = nf\n",
        "\n",
        "        L = laplacian_regularization(edge_index, data.num_nodes)\n",
        "        for layer in self.layers:\n",
        "            xs = layer(xs, edge_index) + xs\n",
        "\n",
        "        out = self.decoder(xs)\n",
        "        if USE_SPECTRAL:\n",
        "            flat = xs.view(xs.size(0), -1)\n",
        "            spec = torch.trace(flat.T @ L @ flat)\n",
        "            spec = torch.nan_to_num(spec, nan=0.0, posinf=1e6, neginf=1e6) / data.num_nodes\n",
        "        else:\n",
        "            spec = torch.tensor(0., device=x.device)\n",
        "        return F.log_softmax(out, dim=1), spec\n",
        "\n",
        "def create_masks(data, tr=0.6, va=0.2):\n",
        "    n = data.num_nodes\n",
        "    perm = torch.randperm(n)\n",
        "    ntr = int(tr*n); nva = int(va*n)\n",
        "    tri = perm[:ntr]; vai = perm[ntr:ntr+nva]; tei = perm[ntr+nva:]\n",
        "    m0 = torch.zeros(n, dtype=torch.bool, device=data.x.device)\n",
        "    data.train_mask = m0.clone().scatter_(0, tri, True)\n",
        "    data.val_mask   = m0.clone().scatter_(0, vai, True)\n",
        "    data.test_mask  = m0.clone().scatter_(0, tei, True)\n",
        "    return data\n",
        "\n",
        "def load_dataset(name):\n",
        "    if name in ['Cora','Citeseer','Pubmed']:\n",
        "        return Planetoid(f'/tmp/{name}',name,transform=NormalizeFeatures())\n",
        "    if name in ['Texas','Wisconsin','Cornell']:\n",
        "        return WebKB(f'/tmp/{name}',name,transform=NormalizeFeatures())\n",
        "    if name in ['Chameleon','Squirrel']:\n",
        "        return WikipediaNetwork(f'/tmp/{name}',name,transform=NormalizeFeatures())\n",
        "    if name=='Film':\n",
        "        return Actor(f'/tmp/Film',transform=NormalizeFeatures())\n",
        "    raise ValueError(name)\n",
        "\n",
        "def objective_deepsheaf(trial, ds_name, device):\n",
        "    d     = trial.suggest_int(\"d\", 8, 64, step=8)\n",
        "    f     = trial.suggest_int(\"f_dim\", 2, 16)\n",
        "    depth = trial.suggest_int(\"depth\", 2, 8)\n",
        "    lr    = trial.suggest_float(\"lr\",1e-4,5e-1,log=True)\n",
        "    wd    = trial.suggest_float(\"weight_decay\",1e-7,1e-1,log=True)\n",
        "    sw    = trial.suggest_float(\"spectral_weight\",1e-7,1e-2,log=True)\n",
        "    epochs= trial.suggest_int(\"epochs\",50,200)\n",
        "\n",
        "    ds   = load_dataset(ds_name)\n",
        "    data = ds[0]\n",
        "    if data.y.dim()>1: data.y = data.y.argmax(dim=1)\n",
        "    if data.x is None: data.x = torch.ones((data.num_nodes,1),device=data.y.device)\n",
        "    data = create_masks(data).to(device)\n",
        "\n",
        "    model = DeepSheafNet(ds.num_node_features, d, f, int(data.y.max())+1,\n",
        "                         spectral_weight=sw, depth=depth).to(device)\n",
        "    opt   = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    crit  = nn.NLLLoss()\n",
        "\n",
        "    best_val = 0.\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train(); opt.zero_grad()\n",
        "        out, reg = model(data)\n",
        "        loss = crit(out[data.train_mask], data.y[data.train_mask]) + sw*reg\n",
        "        loss = torch.nan_to_num(loss, nan=0.0, posinf=1e6, neginf=1e6)\n",
        "        loss.backward()\n",
        "        gn = torch.sqrt(sum((p.grad.norm(2)**2 for p in model.parameters() if p.grad is not None)))\n",
        "        opt.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(data)[0].argmax(dim=1)\n",
        "            tr_acc = (pred[data.train_mask]==data.y[data.train_mask]).float().mean().item()\n",
        "            va_acc = (pred[data.val_mask]  ==data.y[data.val_mask]).float().mean().item()\n",
        "        if va_acc>best_val: best_val=va_acc\n",
        "\n",
        "        if epoch==1 or epoch%20==0:\n",
        "            print(f\"[{ds_name}] ep {epoch:03d} | loss {loss:.4f} | grad {gn:.4e} | train {tr_acc:.3f} | val {va_acc:.3f}\")\n",
        "\n",
        "        trial.report(best_val, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    return best_val\n",
        "\n",
        "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "datasets = [\"Texas\",\"Wisconsin\",\"Cornell\",\n",
        "             \"Chameleon\",\"Squirrel\",\"Film\",\n",
        "             \"Cora\",\"Citeseer\",\"Pubmed\"]\n",
        "results = {}\n",
        "\n",
        "for name in datasets:\n",
        "    print(f\"\\n{name} (DeepSheafNet+PH)\")\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda t: objective_deepsheaf(t, name, device), n_trials=10)\n",
        "    bv, bp = study.best_value, study.best_params\n",
        "    print(f\"\\u2192 Best val acc: {bv:.4f} | params: {bp}\")\n",
        "    results[name] = (bv, bp)\n",
        "\n",
        "print(\"\\nFinal Results\")\n",
        "for nm,(bv,bp) in results.items():\n",
        "    print(f\"{nm:12s} | Val Acc: {bv:.4f} | Params: {bp}\")"
      ],
      "metadata": {
        "id": "K2muboplUCHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "example of start model with parametrs after optuna end cycle."
      ],
      "metadata": {
        "id": "iCQXk4NsVUpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch_geometric.datasets import WikipediaNetwork, WebKB\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "import gudhi\n",
        "from gudhi.representations import PersistenceImage\n",
        "\n",
        "def laplacian_regularization(edge_index, num_nodes):\n",
        "    row, col = edge_index\n",
        "    deg = torch.bincount(row, minlength=num_nodes).float()\n",
        "    D_inv_sqrt = torch.diag(torch.pow(deg, -0.5))\n",
        "    A = torch.zeros(num_nodes, num_nodes, device=edge_index.device)\n",
        "    A[row, col] = 1\n",
        "    return torch.eye(num_nodes, device=edge_index.device) - D_inv_sqrt @ A @ D_inv_sqrt\n",
        "\n",
        "def compute_local_PH(x, edge_index, k=5, maxdim=1):\n",
        "    row, col = edge_index\n",
        "    N = x.size(0)\n",
        "    x_np = x.detach().cpu().numpy()\n",
        "    neighs = [[] for _ in range(N)]\n",
        "    for i,j in zip(row.tolist(), col.tolist()):\n",
        "        neighs[i].append(j)\n",
        "    diagrams = []\n",
        "    for i in range(N):\n",
        "        patch_pts = [x_np[i]] + [x_np[j] for j in neighs[i][:k]]\n",
        "        patch = np.stack(patch_pts, axis=0)\n",
        "        rips = gudhi.RipsComplex(points=patch)\n",
        "        st = rips.create_simplex_tree(max_dimension=maxdim)\n",
        "        st.compute_persistence()\n",
        "        diag = st.persistence_intervals_in_dimension(1)\n",
        "        if len(diag) == 0:\n",
        "            diag = np.array([[0.0, 0.0]])\n",
        "        diagrams.append(diag)\n",
        "    return diagrams\n",
        "\n",
        "def get_persistence_vectors(diagrams, resolution=(5,5)):\n",
        "    PI = PersistenceImage(bandwidth=1.0, weight=lambda pt: pt[1]-pt[0], resolution=resolution)\n",
        "    vecs = PI.fit_transform(diagrams)\n",
        "    vecs = np.nan_to_num(vecs, nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "    return torch.tensor(vecs, dtype=torch.float)\n",
        "\n",
        "def create_masks(data, tr=0.6, va=0.2):\n",
        "    n = data.num_nodes\n",
        "    perm = torch.randperm(n)\n",
        "    ntr = int(tr*n); nva = int(va*n)\n",
        "    tri = perm[:ntr]; vai = perm[ntr:ntr+nva]; tei = perm[ntr+nva:]\n",
        "    m0 = torch.zeros(n, dtype=torch.bool, device=data.x.device)\n",
        "    data.train_mask = m0.clone().scatter_(0, tri, True)\n",
        "    data.val_mask   = m0.clone().scatter_(0, vai, True)\n",
        "    data.test_mask  = m0.clone().scatter_(0, tei, True)\n",
        "    return data\n",
        "\n",
        "class DeepSheafLayer(nn.Module):\n",
        "    def __init__(self, d, f):\n",
        "        super().__init__()\n",
        "        self.d, self.f = d, f\n",
        "        self.lin = nn.Linear(d * f, d * f)\n",
        "        self.ln = nn.LayerNorm([d, f])\n",
        "    def forward(self, x, edge_index):\n",
        "        N = x.size(0)\n",
        "        flat = x.view(N, -1)\n",
        "        out = self.lin(flat).view(N, self.d, self.f)\n",
        "        row, col = edge_index\n",
        "        agg = torch.zeros_like(out)\n",
        "        agg.index_add_(0, row, out[col])\n",
        "        deg = torch.bincount(row, minlength=N).clamp(min=1).float().view(-1,1,1)\n",
        "        return self.ln(F.relu(agg / deg))\n",
        "\n",
        "class DeepSheafNet(nn.Module):\n",
        "    def __init__(self, in_dim, d, f, out_dim, depth=3, spectral_weight=1e-5, ph_dim=25):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Linear(in_dim, d)\n",
        "        self.spectral_weight = spectral_weight\n",
        "        node_dim = d + ph_dim\n",
        "        self.layers = nn.ModuleList([DeepSheafLayer(d, f) for _ in range(depth)])\n",
        "        self.decoder = nn.Sequential(nn.Flatten(), nn.Linear(d * f, out_dim))\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        nf = self.encoder(x)\n",
        "        nf = torch.nan_to_num(nf, nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "        x_sheaf = nf.unsqueeze(2).repeat(1, 1, self.layers[0].f)\n",
        "\n",
        "        ph_diags = compute_local_PH(nf, edge_index)\n",
        "        ph_vecs = get_persistence_vectors(ph_diags).to(x.device)\n",
        "        ph_vecs = torch.nan_to_num(ph_vecs, nan=0.0, posinf=1e3, neginf=-1e3)\n",
        "        node_feat = torch.cat([nf, ph_vecs], dim=1)\n",
        "\n",
        "        L = laplacian_regularization(edge_index, data.num_nodes)\n",
        "        for layer in self.layers:\n",
        "            x_sheaf = layer(x_sheaf, edge_index) + x_sheaf\n",
        "\n",
        "        out = self.decoder(x_sheaf)\n",
        "        if self.spectral_weight > 0:\n",
        "            flat = x_sheaf.view(x_sheaf.size(0), -1)\n",
        "            spectral = torch.trace(flat.T @ L @ flat) / data.num_nodes\n",
        "        else:\n",
        "            spectral = torch.tensor(0., device=x.device)\n",
        "        return F.log_softmax(out, dim=1), spectral\n",
        "\n",
        "torch.manual_seed(42); np.random.seed(42); random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "params = {\n",
        "    'd': 8, 'f_dim': 16, 'depth': 2, 'lr': 0.07876328739451417, 'weight_decay': 0.00035039182415240504, 'spectral_weight': 3.6910574695954574e-07, 'epochs': 100}\n",
        "\n",
        "data = WebKB(root=\"/tmp/Texas\", name=\"Texas\", transform=NormalizeFeatures())[0]\n",
        "if data.y.dim() > 1:\n",
        "    data.y = data.y.argmax(dim=1)\n",
        "if data.x is None:\n",
        "    data.x = torch.ones((data.num_nodes, 1))\n",
        "data.edge_index = to_undirected(data.edge_index)\n",
        "data = create_masks(data).to(device)\n",
        "\n",
        "model = DeepSheafNet(\n",
        "    in_dim=data.num_node_features,\n",
        "    d=params['d'],\n",
        "    f=params['f_dim'],\n",
        "    out_dim=int(data.y.max()) + 1,\n",
        "    depth=params['depth'],\n",
        "    spectral_weight=params['spectral_weight']\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "for epoch in range(1, params['epochs'] + 1):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out, reg = model(data)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask]) + params['spectral_weight'] * reg\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out, _ = model(data)\n",
        "        pred = out.argmax(dim=1)\n",
        "        tr = (pred[data.train_mask] == data.y[data.train_mask]).float().mean().item()\n",
        "        va = (pred[data.val_mask]   == data.y[data.val_mask]).float().mean().item()\n",
        "        te = (pred[data.test_mask]  == data.y[data.test_mask]).float().mean().item()\n",
        "\n",
        "\n",
        "    print(f\"[Texas] ep {epoch:03d} | loss {loss:.4f} | train {tr:.3f} | val {va:.3f} | test {te:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UvsFSn7Umo6",
        "outputId": "0fe0b382-094e-449f-dafb-0e3ab469fa9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Texas] ep 001 | loss 1.3549 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 002 | loss 1.6624 | train 0.202 | val 0.083 | test 0.132\n",
            "[Texas] ep 003 | loss 6.8751 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 004 | loss 6.4649 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 005 | loss 2.7531 | train 0.156 | val 0.139 | test 0.289\n",
            "[Texas] ep 006 | loss 8.3454 | train 0.202 | val 0.083 | test 0.132\n",
            "[Texas] ep 007 | loss 6.6320 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 008 | loss 3.5822 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 009 | loss 4.6833 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 010 | loss 3.8001 | train 0.092 | val 0.056 | test 0.158\n",
            "[Texas] ep 011 | loss 4.0056 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 012 | loss 1.7195 | train 0.202 | val 0.083 | test 0.132\n",
            "[Texas] ep 013 | loss 2.9026 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 014 | loss 1.7829 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 015 | loss 2.1022 | train 0.312 | val 0.167 | test 0.316\n",
            "[Texas] ep 016 | loss 2.5923 | train 0.688 | val 0.833 | test 0.553\n",
            "[Texas] ep 017 | loss 2.4185 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 018 | loss 2.0665 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 019 | loss 1.6567 | train 0.202 | val 0.083 | test 0.132\n",
            "[Texas] ep 020 | loss 1.9213 | train 0.716 | val 0.556 | test 0.474\n",
            "[Texas] ep 021 | loss 1.3788 | train 0.541 | val 0.722 | test 0.421\n",
            "[Texas] ep 022 | loss 1.1330 | train 0.615 | val 0.722 | test 0.447\n",
            "[Texas] ep 023 | loss 1.2963 | train 0.532 | val 0.278 | test 0.289\n",
            "[Texas] ep 024 | loss 1.5685 | train 0.706 | val 0.750 | test 0.553\n",
            "[Texas] ep 025 | loss 1.1975 | train 0.697 | val 0.861 | test 0.684\n",
            "[Texas] ep 026 | loss 0.9085 | train 0.771 | val 0.861 | test 0.711\n",
            "[Texas] ep 027 | loss 0.8321 | train 0.789 | val 0.500 | test 0.421\n",
            "[Texas] ep 028 | loss 0.9539 | train 0.807 | val 0.750 | test 0.526\n",
            "[Texas] ep 029 | loss 0.9139 | train 0.734 | val 0.833 | test 0.526\n",
            "[Texas] ep 030 | loss 0.8319 | train 0.725 | val 0.833 | test 0.632\n",
            "[Texas] ep 031 | loss 0.7750 | train 0.761 | val 0.861 | test 0.711\n",
            "[Texas] ep 032 | loss 0.6454 | train 0.881 | val 0.889 | test 0.737\n",
            "[Texas] ep 033 | loss 0.5370 | train 0.899 | val 0.778 | test 0.684\n",
            "[Texas] ep 034 | loss 0.4967 | train 0.954 | val 0.778 | test 0.711\n",
            "[Texas] ep 035 | loss 0.4723 | train 0.982 | val 0.889 | test 0.842\n",
            "[Texas] ep 036 | loss 0.4103 | train 0.862 | val 0.778 | test 0.579\n",
            "[Texas] ep 037 | loss 0.3910 | train 0.844 | val 0.750 | test 0.579\n",
            "[Texas] ep 038 | loss 0.3872 | train 0.936 | val 0.806 | test 0.579\n",
            "[Texas] ep 039 | loss 0.3406 | train 0.945 | val 0.861 | test 0.658\n",
            "[Texas] ep 040 | loss 0.2995 | train 0.954 | val 0.750 | test 0.763\n",
            "[Texas] ep 041 | loss 0.2786 | train 0.954 | val 0.778 | test 0.737\n",
            "[Texas] ep 042 | loss 0.2484 | train 0.963 | val 0.889 | test 0.789\n",
            "[Texas] ep 043 | loss 0.2222 | train 0.963 | val 0.861 | test 0.789\n",
            "[Texas] ep 044 | loss 0.2131 | train 0.963 | val 0.861 | test 0.737\n",
            "[Texas] ep 045 | loss 0.2058 | train 0.972 | val 0.889 | test 0.737\n",
            "[Texas] ep 046 | loss 0.1850 | train 0.991 | val 0.889 | test 0.816\n",
            "[Texas] ep 047 | loss 0.1538 | train 1.000 | val 0.917 | test 0.842\n",
            "[Texas] ep 048 | loss 0.1286 | train 1.000 | val 0.889 | test 0.842\n",
            "[Texas] ep 049 | loss 0.1200 | train 1.000 | val 0.889 | test 0.842\n",
            "[Texas] ep 050 | loss 0.1233 | train 1.000 | val 0.889 | test 0.816\n",
            "[Texas] ep 051 | loss 0.1263 | train 1.000 | val 0.889 | test 0.816\n",
            "[Texas] ep 052 | loss 0.1206 | train 1.000 | val 0.944 | test 0.816\n",
            "[Texas] ep 053 | loss 0.1090 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 054 | loss 0.0977 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 055 | loss 0.0892 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 056 | loss 0.0844 | train 1.000 | val 0.944 | test 0.816\n",
            "[Texas] ep 057 | loss 0.0828 | train 1.000 | val 0.944 | test 0.816\n",
            "[Texas] ep 058 | loss 0.0831 | train 1.000 | val 0.944 | test 0.816\n",
            "[Texas] ep 059 | loss 0.0829 | train 1.000 | val 0.944 | test 0.816\n",
            "[Texas] ep 060 | loss 0.0808 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 061 | loss 0.0770 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 062 | loss 0.0728 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 063 | loss 0.0694 | train 1.000 | val 0.944 | test 0.895\n",
            "[Texas] ep 064 | loss 0.0679 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 065 | loss 0.0680 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 066 | loss 0.0685 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 067 | loss 0.0679 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 068 | loss 0.0659 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 069 | loss 0.0634 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 070 | loss 0.0612 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 071 | loss 0.0597 | train 1.000 | val 0.944 | test 0.816\n",
            "[Texas] ep 072 | loss 0.0588 | train 1.000 | val 0.944 | test 0.816\n",
            "[Texas] ep 073 | loss 0.0579 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 074 | loss 0.0569 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 075 | loss 0.0558 | train 1.000 | val 0.944 | test 0.842\n",
            "[Texas] ep 076 | loss 0.0546 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 077 | loss 0.0536 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 078 | loss 0.0525 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 079 | loss 0.0514 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 080 | loss 0.0505 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 081 | loss 0.0497 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 082 | loss 0.0490 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 083 | loss 0.0479 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 084 | loss 0.0468 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 085 | loss 0.0458 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 086 | loss 0.0449 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 087 | loss 0.0442 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 088 | loss 0.0434 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 089 | loss 0.0426 | train 1.000 | val 0.944 | test 0.868\n",
            "[Texas] ep 090 | loss 0.0416 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 091 | loss 0.0407 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 092 | loss 0.0399 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 093 | loss 0.0391 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 094 | loss 0.0383 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 095 | loss 0.0375 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 096 | loss 0.0366 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 097 | loss 0.0358 | train 1.000 | val 0.972 | test 0.868\n",
            "[Texas] ep 098 | loss 0.0349 | train 1.000 | val 0.972 | test 0.842\n",
            "[Texas] ep 099 | loss 0.0340 | train 1.000 | val 0.972 | test 0.842\n",
            "[Texas] ep 100 | loss 0.0329 | train 1.000 | val 0.972 | test 0.842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of MLP"
      ],
      "metadata": {
        "id": "ymxbeDf9UiHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork, Actor\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "\n",
        "\n",
        "class MLPNodeClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        layers = [nn.Linear(in_channels, hidden_channels), nn.ReLU()]\n",
        "        for _ in range(num_layers - 1):\n",
        "            layers += [nn.Linear(hidden_channels, hidden_channels), nn.ReLU()]\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "        self.classifier = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.classifier(self.mlp(x))\n",
        "\n",
        "def load_dataset(name):\n",
        "    if name in ['Cora','Citeseer','Pubmed']:\n",
        "        return Planetoid(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name in ['Texas','Wisconsin','Cornell']:\n",
        "        return WebKB(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name in ['Chameleon','Squirrel']:\n",
        "        return WikipediaNetwork(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name == 'Film':\n",
        "        return Actor(f'/tmp/Film', transform=NormalizeFeatures())\n",
        "    raise ValueError(name)\n",
        "\n",
        "\n",
        "def create_masks(data, train_frac=0.6, val_frac=0.2):\n",
        "    num_nodes = data.num_nodes\n",
        "    perm = torch.randperm(num_nodes)\n",
        "    n_train = int(train_frac * num_nodes)\n",
        "    n_val   = int(val_frac   * num_nodes)\n",
        "    train_idx = perm[:n_train]\n",
        "    val_idx   = perm[n_train:n_train+n_val]\n",
        "    test_idx  = perm[n_train+n_val:]\n",
        "    data.train_mask = torch.zeros(num_nodes, dtype=torch.bool, device=data.x.device)\n",
        "    data.val_mask   = data.train_mask.clone()\n",
        "    data.test_mask  = data.train_mask.clone()\n",
        "    data.train_mask[train_idx] = True\n",
        "    data.val_mask[  val_idx]   = True\n",
        "    data.test_mask[ test_idx]  = True\n",
        "    return data\n",
        "\n",
        "\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x)\n",
        "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model(data.x)\n",
        "        pred = out.argmax(dim=1)\n",
        "        accs = []\n",
        "        for mask in (data.train_mask, data.val_mask, data.test_mask):\n",
        "            acc = (pred[mask] == data.y[mask]).sum().item() / mask.sum().item()\n",
        "            accs.append(acc)\n",
        "    return accs\n",
        "\n",
        "\n",
        "def objective_mlp(trial, dataset_name, device):\n",
        "    hidden = trial.suggest_int(\"hidden_channels\", 32, 256, step=32)\n",
        "    layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
        "    lr     = trial.suggest_float(\"lr\", 1e-3, 1e-1, log=True)\n",
        "    wd     = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    epochs = trial.suggest_int(\"epochs\", 50, 200)\n",
        "\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    data = dataset[0]\n",
        "\n",
        "    if data.y.dim() > 1:\n",
        "        data.y = data.y.argmax(dim=1)\n",
        "    if data.x is None:\n",
        "        data.x = torch.ones((data.num_nodes, 1), dtype=torch.float)\n",
        "\n",
        "    data = create_masks(data)\n",
        "    data = data.to(device)\n",
        "\n",
        "    in_ch = dataset.num_node_features or data.x.size(1)\n",
        "    out_ch = int(torch.unique(data.y).numel())\n",
        "\n",
        "    model = MLPNodeClassifier(in_ch, hidden, layers, out_ch).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val = 0.\n",
        "    best_test = 0.\n",
        "    for _ in range(epochs):\n",
        "        train(model, data, optimizer, criterion)\n",
        "        train_acc, val_acc, test_acc = evaluate(model, data)\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_test = test_acc\n",
        "        trial.report(best_val, _)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "    return best_val\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "datasets = [\"Texas\", \"Wisconsin\", \"Cornell\",\n",
        "                \"Chameleon\", \"Squirrel\", \"Film\",\n",
        "                \"Cora\", \"Citeseer\", \"Pubmed\"]\n",
        "\n",
        "results = {}\n",
        "for name in datasets:\n",
        "    print(f\"\\nDataset: {name} (MLP)\")\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda t: objective_mlp(t, name, device), n_trials=20)\n",
        "\n",
        "    best_val = study.best_value\n",
        "    best_params = study.best_params\n",
        "    print(f\"Best val acc: {best_val:.4f}\")\n",
        "    print(\"Best params:\", best_params)\n",
        "    results[name] = (best_val, best_params)\n",
        "\n",
        "print(\"\\nFinal MLP Results\")\n",
        "for name, (val, params) in results.items():\n",
        "    print(f\"{name:12s} | Val Acc: {val:.4f} | Params: {params}\")"
      ],
      "metadata": {
        "id": "th84EaKmUh8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of GAT"
      ],
      "metadata": {
        "id": "XUfm2hgAV2kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork, Actor\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "\n",
        "class GATNodeClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, heads, num_classes):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GATConv(in_channels, hidden_channels, heads=heads))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GATConv(hidden_channels * heads, hidden_channels, heads=heads))\n",
        "        self.final = nn.Linear(hidden_channels * heads, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = F.elu(conv(x, edge_index))\n",
        "        return self.final(x)\n",
        "\n",
        "def create_masks(data, train_frac=0.6, val_frac=0.2):\n",
        "    n = data.num_nodes\n",
        "    perm = torch.randperm(n)\n",
        "    n_train = int(train_frac * n)\n",
        "    n_val   = int(val_frac   * n)\n",
        "    idx_train = perm[:n_train]\n",
        "    idx_val   = perm[n_train:n_train+n_val]\n",
        "    idx_test  = perm[n_train+n_val:]\n",
        "    mask = lambda idx: torch.zeros(n, dtype=torch.bool, device=data.x.device).scatter_(0, idx, True)\n",
        "    data.train_mask = mask(idx_train)\n",
        "    data.val_mask   = mask(idx_val)\n",
        "    data.test_mask  = mask(idx_test)\n",
        "    return data\n",
        "\n",
        "def load_dataset(name):\n",
        "    if name in ['Cora','Citeseer','Pubmed']:\n",
        "        return Planetoid(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name in ['Texas','Wisconsin','Cornell']:\n",
        "        return WebKB(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name in ['Chameleon','Squirrel']:\n",
        "        return WikipediaNetwork(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name == 'Film':\n",
        "        return Actor(f'/tmp/Film', transform=NormalizeFeatures())\n",
        "    raise ValueError(name)\n",
        "\n",
        "def objective_gat(trial, dataset_name, device):\n",
        "    hidden = trial.suggest_int(\"hidden_channels\", 32, 256, step=32)\n",
        "    layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
        "    heads  = trial.suggest_int(\"heads\", 1, 8)\n",
        "    lr     = trial.suggest_float(\"lr\", 1e-3, 1e-1, log=True)\n",
        "    wd     = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    epochs = trial.suggest_int(\"epochs\", 50, 200)\n",
        "\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    data = dataset[0]\n",
        "\n",
        "    if data.y.dim() > 1:\n",
        "        data.y = data.y.argmax(dim=1)\n",
        "    if data.x is None:\n",
        "        data.x = torch.ones((data.num_nodes, 1), dtype=torch.float)\n",
        "\n",
        "    data = create_masks(data)\n",
        "    data = data.to(device)\n",
        "\n",
        "    in_ch = dataset.num_node_features or data.x.size(1)\n",
        "    out_ch = int(torch.unique(data.y).numel())\n",
        "\n",
        "    model = GATNodeClassifier(in_ch, hidden, layers, heads, out_ch).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val, best_test = 0., 0.\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(data.x, data.edge_index).argmax(dim=1)\n",
        "            val_acc  = (pred[data.val_mask]  == data.y[data.val_mask]).float().mean().item()\n",
        "            test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
        "\n",
        "        if val_acc > best_val:\n",
        "            best_val, best_test = val_acc, test_acc\n",
        "\n",
        "        trial.report(best_val, _)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "datasets = [\"Texas\", \"Wisconsin\", \"Cornell\",\n",
        "                \"Chameleon\", \"Squirrel\", \"Film\",\n",
        "                \"Cora\", \"Citeseer\", \"Pubmed\"]\n",
        "\n",
        "results = {}\n",
        "for name in datasets:\n",
        "    print(f\"\\n Dataset: {name} (GAT)\")\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda t: objective_gat(t, name, device), n_trials=20)\n",
        "\n",
        "    best_val = study.best_value\n",
        "    best_params = study.best_params\n",
        "    print(f\"Best val acc: {best_val:.4f}\")\n",
        "    print(\"Best params:\", best_params)\n",
        "    results[name] = (best_val, best_params)\n",
        "\n",
        "print(\"\\nFinal GAT Results\")\n",
        "for name, (val, params) in results.items():\n",
        "    print(f\"{name:12s} | Val Acc: {val:.4f} | Params: {params}\")"
      ],
      "metadata": {
        "id": "kWHyF1NxV_7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of GCN"
      ],
      "metadata": {
        "id": "p2MISlJqWAga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import optuna\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork, Actor\n",
        "from torch_geometric.transforms import NormalizeFeatures\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "\n",
        "class GCNNodeClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_channels, hidden_channels))\n",
        "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        for conv in self.convs:\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "def create_masks(data, train_frac=0.6, val_frac=0.2):\n",
        "    n = data.num_nodes\n",
        "    perm = torch.randperm(n)\n",
        "    n_train = int(train_frac * n)\n",
        "    n_val   = int(val_frac * n)\n",
        "    idx_train = perm[:n_train]\n",
        "    idx_val   = perm[n_train:n_train+n_val]\n",
        "    idx_test  = perm[n_train+n_val:]\n",
        "    mask = lambda idx: torch.zeros(n, dtype=torch.bool, device=data.x.device).scatter_(0, idx, True)\n",
        "    data.train_mask = mask(idx_train)\n",
        "    data.val_mask   = mask(idx_val)\n",
        "    data.test_mask  = mask(idx_test)\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_dataset(name):\n",
        "    if name in ['Cora','Citeseer','Pubmed']:\n",
        "        return Planetoid(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name in ['Texas','Wisconsin','Cornell']:\n",
        "        return WebKB(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name in ['Chameleon','Squirrel']:\n",
        "        return WikipediaNetwork(f'/tmp/{name}', name, transform=NormalizeFeatures())\n",
        "    if name == 'Film':\n",
        "        return Actor(f'/tmp/Film', transform=NormalizeFeatures())\n",
        "    raise ValueError(name)\n",
        "\n",
        "-\n",
        "def objective_gcn(trial, ds_name, device):\n",
        "    hidden = trial.suggest_int(\"hidden_channels\", 32, 256, step=32)\n",
        "    layers = trial.suggest_int(\"num_layers\", 1, 4)\n",
        "    lr     = trial.suggest_float(\"lr\", 1e-3, 1e-1, log=True)\n",
        "    wd     = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
        "    epochs = trial.suggest_int(\"epochs\", 50, 200)\n",
        "\n",
        "    ds = load_dataset(ds_name)\n",
        "    data = ds[0]\n",
        "\n",
        "    if data.y.dim() > 1:\n",
        "        data.y = data.y.argmax(dim=1)\n",
        "    if data.x is None:\n",
        "        data.x = torch.ones((data.num_nodes, 1), dtype=torch.float)\n",
        "\n",
        "    data = create_masks(data)\n",
        "    data = data.to(device)\n",
        "\n",
        "    in_ch = ds.num_node_features or data.x.size(1)\n",
        "    num_cls = int(torch.unique(data.y).numel())\n",
        "\n",
        "    model = GCNNodeClassifier(in_ch, hidden, layers, num_cls).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    crit = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val, best_test = 0., 0.\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        opt.zero_grad()\n",
        "        out = model(data.x, data.edge_index)\n",
        "        loss = crit(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(data.x, data.edge_index).argmax(dim=1)\n",
        "            val_acc  = (pred[data.val_mask]  == data.y[data.val_mask]).float().mean().item()\n",
        "            test_acc = (pred[data.test_mask] == data.y[data.test_mask]).float().mean().item()\n",
        "\n",
        "        if val_acc > best_val:\n",
        "            best_val, best_test = val_acc, test_acc\n",
        "\n",
        "        trial.report(best_val, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return best_val\n",
        "\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "datasets = [\"Texas\", \"Wisconsin\", \"Cornell\",\n",
        "                \"Chameleon\", \"Squirrel\", \"Film\",\n",
        "                \"Cora\", \"Citeseer\", \"Pubmed\"]\n",
        "\n",
        "results = {}\n",
        "for name in datasets:\n",
        "    print(f\"\\nDataset: {name} (GCN)\")\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(lambda t: objective_gcn(t, name, device), n_trials=20)\n",
        "\n",
        "    best_val = study.best_value\n",
        "    best_params = study.best_params\n",
        "    print(f\"Best val acc: {best_val:.4f}\")\n",
        "    print(\"Best params:\", best_params)\n",
        "    results[name] = (best_val, best_params)\n",
        "\n",
        "print(\"\\nFinal GCN Results\")\n",
        "for name, (val, params) in results.items():\n",
        "    print(f\"{name:12s} | Val Acc: {val:.4f} | Params: {params}\")"
      ],
      "metadata": {
        "id": "zOXjFTvNWCM2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}